{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K arm bandit for recommendation system\n",
    "Considering a recommendation system, we need to recommend relevant items to the user. 1 item could have n relevant items. K arm bandit takes an action based on the environment whereas while considering a recommendation system, it needs to identify the needs of the user and take and action.\n",
    "\n",
    "Good recommendations are rewarded, and bad recommendations are punished in case of RL based recommendation system.\n",
    "\n",
    "Unlike a Robot cleaning dirt problem, where the robot will not visit a tile that is not cleaned, in a recommendation system, visited item can be revisited (recommended)\n",
    "\n",
    "The user here will give feedback (reward) if they find the recommendation relevant. Instead of a probability distribution from which rewards are drawn, we use a human to give a reward depending on how well the recommendation is. Userâ€™s feedback is subjective and depends on a lot of external factors. The reward cannot be learnt in this case. You can never predict what reward you will get.\n",
    "\n",
    "Initially the exploration phase happens where the algorithm will recommend random items to the user and the user had to reward/punish accordingly so that it learns, which item, when recommended, yields a good reward.\n",
    "\n",
    "A recommendation given by the algorithm is considered arm here.\n",
    "\n",
    "By recommending items in the exploration phase, the algorithm tends to get an idea regarding which action taken for a particular item yields good rewards.\n",
    "\n",
    "For example, let's say A and B are relevant and C is irrelevant to A. When started from A, we explore B and C. B gets rewarded more than C, in this way, while exploring, the algorithms learns that A and B are relevant, and C is irrelevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmBanditRecommendation:\n",
    "    def __init__(self, k, epsilon=0.1):\n",
    "        self.k = k \n",
    "        self.epsilon = epsilon  \n",
    "        self.q_values = np.zeros(k)  \n",
    "        self.action_counts = np.zeros(k)  # number of times each arm was selected\n",
    "\n",
    "    def select_action(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            #explore\n",
    "            action = random.randint(0, self.k - 1)\n",
    "        else:\n",
    "            #exploit\n",
    "            action = np.argmax(self.q_values)\n",
    "        return action\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        #action update\n",
    "        self.action_counts[action] += 1\n",
    "\n",
    "        n = self.action_counts[action]\n",
    "        self.q_values[action] += (1 / n) * (reward - self.q_values[action])\n",
    "\n",
    "    def recommend_item(self, items):\n",
    "        action = self.select_action()\n",
    "        return items[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_arm_bandit = KArmBanditRecommendation(k=10, epsilon=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item_A',\n",
       " 'Item_B',\n",
       " 'Item_C',\n",
       " 'Item_D',\n",
       " 'Item_E',\n",
       " 'Item_F',\n",
       " 'Item_G',\n",
       " 'Item_H',\n",
       " 'Item_I',\n",
       " 'Item_J']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [f'Item_{chr(65 + i)}' for i in range(10)]\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback : 1 for good recommendation, 0 for bad recommendation\\\n",
    "We assume ItemA and ItemF to be more relevant (good recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended: Item_C\n",
      "Recommended: Item_G\n",
      "Recommended: Item_F\n",
      "Recommended: Item_F\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_H\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_B\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_D\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_G\n",
      "Recommended: Item_B\n",
      "Recommended: Item_A\n",
      "Recommended: Item_G\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_H\n",
      "Recommended: Item_H\n",
      "Recommended: Item_H\n",
      "Recommended: Item_A\n",
      "Recommended: Item_C\n",
      "Recommended: Item_A\n",
      "Recommended: Item_G\n",
      "Recommended: Item_A\n",
      "Recommended: Item_C\n",
      "Recommended: Item_A\n",
      "Recommended: Item_G\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n",
      "Recommended: Item_A\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    # Recommend an item\n",
    "    recommended_item = k_arm_bandit.recommend_item(items)\n",
    "    print(f\"Recommended: {recommended_item}\")\n",
    "\n",
    "    # Simulate user feedback (1 for good recommendation, 0 for bad recommendation)\n",
    "    # For simplicity, we assume some items are more relevant than others\n",
    "    reward = 1 if recommended_item in ['Item_A', 'Item_F'] else 0  # User rewards Item_A and Item_B\n",
    "\n",
    "    # Update estimates with the observed reward\n",
    "    action = items.index(recommended_item)\n",
    "    k_arm_bandit.update_estimates(action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated value for Item_A: 1.0\n",
      "Estimated value for Item_B: 0.0\n",
      "Estimated value for Item_C: 0.0\n",
      "Estimated value for Item_D: 0.0\n",
      "Estimated value for Item_E: 0.0\n",
      "Estimated value for Item_F: 1.0\n",
      "Estimated value for Item_G: 0.0\n",
      "Estimated value for Item_H: 0.0\n",
      "Estimated value for Item_I: 0.0\n",
      "Estimated value for Item_J: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(items):\n",
    "    print(f\"Estimated value for {item}: {k_arm_bandit.q_values[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
